现阶段

1025889

现阶段  100万数据 transaction----insert---20w---120s

现阶段  120万数据  transaction----saveAll---20w---133s

现阶段  140万数据 ----saveAll---20w---124s

现阶段  140万数据 ----saveAll---20w---124s



简介：数据量 5225889，前些天，被人问到怎么优化千万级别数据量怎么优化limit的问题，说实话，我没做过，不是没有千万数据量，而是我们这边千万数据量不会用mysql来分页~，就算有，也不会是一张表千万数据量，但是被问到这个问题，我只能很遗憾的说，我不知道，没做过.



```sql
CREATE TABLE `big_data` (
  `id` bigint(20) NOT NULL,
  `big_decimal` decimal(19,2) DEFAULT NULL,
  `char_type` char(10) NOT NULL DEFAULT '',
  `date_time` datetime DEFAULT NULL,
  `double_num` double DEFAULT NULL,
  `float_num` float DEFAULT NULL,
  `int_num` int(11) DEFAULT NULL,
  `varchar_type` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
```





表数据量500万多一点点

```sql
//其中id为主键,执行时间12.85s
select b.id from big_data b limit 5000000,10
```

执行结果如下

![屏幕快照 2019-08-19 下午4.23.03](/Users/jzue/Desktop/blog_file/文章图片/500万数据优化/屏幕快照 2019-08-19 下午4.23.03.png)

然后我执行一下show profiles，看下到底是哪里耗时间了；

![屏幕快照 2019-08-19 下午4.29.28](/Users/jzue/Desktop/blog_file/文章图片/500万数据优化/屏幕快照 2019-08-19 下午4.29.28.png)



主要时间耗费在sending data:

过长的常见原因

1、查询字段过多 使用了 SELECT * FROM　TABLE

2、查询数据过多  比如 LIMIT 0,100000

3、查询数据不多 但是偏移量大 比如 LIMIT 1000,10



如果是 第一种情况  修改sql，只查对应的需要的字段

如果是第二种情况  这种数据量过多的，我觉得~应该属于很难解决的 

如果是第三种情况  可以采用比如用ID 排序 可以 用ID>XXXX （between） 代替 limit 1000


```sql
select b.id from big_data b where b.id>=807584646823955 and b.id<=807584646823964
```



![屏幕快照 2019-08-19 下午4.27.18](/Users/jzue/Desktop/blog_file/文章图片/500万数据优化/屏幕快照 2019-08-19 下午4.27.18.png)



```sql
select b.id from big_data b where b.id between 807584646823955 and 807584646823964;
```



![屏幕快照 2019-08-19 下午4.34.49](/Users/jzue/Desktop/blog_file/文章图片/500万数据优化/屏幕快照 2019-08-19 下午4.34.49.png)

上面的优化方法是挺好的，但是，也不实用啊~，对不对，如果你要维护一个完全不间断的自增的一个排序字段才可以~不然，你不可能根据一个临界点计算出另一个临界点，这样的话~对于没有删除操作的表，确实可以用，但是没有删除操作…..这也太坑了吧

```sql
 // 这种方式，需要我们每次分页的时候，需要将上一页最后的一个id传过来，部分场景适用,因为分页的时候，谁也不能保证用户会一页一页的连续的看，万一用户直接跳到某一页了呢，但是在预加载的地方任然有其适用性
 select id from big_data b where b.id>807584646823954 order by id asc limit 10;
 // 这种方式由于offsize过大，会有提升，效果一般
 select id from big_data where id>=(select id from big_data limit 5000000,1) order by id asc limit 10;
 // 过半的反向查~效果还行~但是，得维护一个count，不然这个count 也很耗时间，特别是where 条件过多的情况，那简直是，毫无适用性，而且order by 也需要做优化，可能还要加索引
 select b.id from big_data b where b.id  order by id desc limit 总数-offset-size,10
 // 另外维护一个分页表~
 // 最后一个方式~数据量这么大，为啥还要mysql分页呢？这个也是我面试的时候被问到这个问题很想说的，这个需求一点都不合理，然后我上面模拟的也是一种很极端的情况，rows 500万+平时应该很少
 
另外：
查询字段一较长字符串的时候，表设计时要为该字段多加一个字段,如，存储网址的字段 
查询的时候，不要直接查询字符串，效率低下，应该查诡该字串的crc32或md5 
```



